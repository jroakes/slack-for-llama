2024-11-16 05:31:28,748 - INFO - Starting training pipeline:
2024-11-16 05:31:28,748 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 05:31:28,748 - INFO - Output directory: ./llama3_finetuned
2024-11-16 05:31:28,748 - INFO - Batch size: 4
2024-11-16 05:31:28,748 - INFO - Training epochs: 3
2024-11-16 05:31:28,748 - INFO - Max sequence length: 2048
2024-11-16 05:31:28,748 - INFO - Quantization: Enabled
2024-11-16 05:31:28,748 - INFO - Quantization type: 4-bit
2024-11-16 05:31:28,749 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 05:31:28,749 - INFO - Using data directory: ./data
2024-11-16 05:31:29,086 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 05:31:31,275 - INFO - Found 62136 messages
2024-11-16 05:31:31,605 - INFO - Grouped messages into 6845 conversations
2024-11-16 05:31:47,657 - WARNING - Skipped 23 examples due to length constraints
2024-11-16 05:31:47,888 - INFO - Created dataset with 23004 training and 2557 test examples
2024-11-16 05:31:47,896 - INFO - Preprocessing completed successfully
2024-11-16 05:31:47,897 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-16 05:31:48,052 - INFO - Starting model fine-tuning...
2024-11-16 05:31:48,176 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-16 05:31:48,265 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-16 05:31:48,266 - INFO - Compute dtype: torch.bfloat16
2024-11-16 05:31:48,266 - INFO - Quantization config: Enabled
2024-11-16 05:31:48,651 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-16 05:31:50,668 - INFO - Successfully loaded model and tokenizer
2024-11-16 05:31:50,669 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-16 05:31:51,179 - INFO - Processing training dataset...
2024-11-16 05:31:58,244 - INFO - Processing evaluation dataset...
2024-11-16 05:32:01,526 - INFO - Starting training...
2024-11-16 05:32:06,456 - ERROR - Training error: '<=' not supported between instances of 'float' and 'str'
2024-11-16 05:35:41,454 - INFO - Starting training pipeline:
2024-11-16 05:35:41,454 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 05:35:41,454 - INFO - Output directory: ./llama3_finetuned
2024-11-16 05:35:41,454 - INFO - Batch size: 4
2024-11-16 05:35:41,454 - INFO - Training epochs: 3
2024-11-16 05:35:41,454 - INFO - Max sequence length: 2048
2024-11-16 05:35:41,454 - INFO - Quantization: Enabled
2024-11-16 05:35:41,455 - INFO - Quantization type: 4-bit
2024-11-16 05:35:41,455 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 05:35:41,455 - INFO - Using data directory: ./data
2024-11-16 05:35:41,798 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 05:35:43,857 - INFO - Found 62136 messages
2024-11-16 05:35:44,191 - INFO - Grouped messages into 6845 conversations
2024-11-16 05:36:00,278 - WARNING - Skipped 23 examples due to length constraints
2024-11-16 05:36:00,509 - INFO - Created dataset with 23004 training and 2557 test examples
2024-11-16 05:36:00,516 - INFO - Preprocessing completed successfully
2024-11-16 05:36:00,518 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-16 05:36:00,670 - INFO - Starting model fine-tuning...
2024-11-16 05:36:00,786 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-16 05:36:00,825 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-16 05:36:00,825 - INFO - Compute dtype: torch.bfloat16
2024-11-16 05:36:00,825 - INFO - Quantization config: Enabled
2024-11-16 05:36:01,205 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-16 05:36:03,093 - INFO - Successfully loaded model and tokenizer
2024-11-16 05:36:03,093 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-16 05:36:03,587 - INFO - Processing training dataset...
2024-11-16 05:36:10,533 - INFO - Processing evaluation dataset...
2024-11-16 05:36:13,682 - INFO - Starting training...
2024-11-16 05:36:18,669 - ERROR - Training error: '<=' not supported between instances of 'float' and 'str'
2024-11-16 05:53:33,731 - INFO - Starting training pipeline:
2024-11-16 05:53:33,731 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 05:53:33,731 - INFO - Output directory: ./llama3_finetuned
2024-11-16 05:53:33,731 - INFO - Batch size: 4
2024-11-16 05:53:33,731 - INFO - Training epochs: 3
2024-11-16 05:53:33,731 - INFO - Max sequence length: 2048
2024-11-16 05:53:33,732 - INFO - Quantization: Enabled
2024-11-16 05:53:33,732 - INFO - Quantization type: 4-bit
2024-11-16 05:53:33,732 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 05:53:33,732 - INFO - Using data directory: ./data
2024-11-16 05:53:34,115 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 05:53:36,308 - INFO - Found 62136 messages
2024-11-16 05:53:36,670 - INFO - Grouped messages into 6845 conversations
2024-11-16 05:53:52,763 - WARNING - Skipped 23 examples due to length constraints
2024-11-16 05:53:52,996 - INFO - Created dataset with 23004 training and 2557 test examples
2024-11-16 05:53:53,003 - INFO - Preprocessing completed successfully
2024-11-16 05:53:53,005 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-16 05:53:53,162 - INFO - Starting model fine-tuning...
2024-11-16 05:53:53,280 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-16 05:53:53,318 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-16 05:53:53,318 - INFO - Compute dtype: torch.bfloat16
2024-11-16 05:53:53,318 - INFO - Quantization config: Enabled
2024-11-16 05:53:53,688 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-16 05:53:55,669 - INFO - Successfully loaded model and tokenizer
2024-11-16 05:53:55,669 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-16 05:54:00,508 - INFO - Starting training...
2024-11-16 06:14:55,568 - INFO - Starting training pipeline:
2024-11-16 06:14:55,569 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 06:14:55,569 - INFO - Output directory: ./llama3_finetuned
2024-11-16 06:14:55,569 - INFO - Batch size: 4
2024-11-16 06:14:55,569 - INFO - Training epochs: 3
2024-11-16 06:14:55,569 - INFO - Max sequence length: 2048
2024-11-16 06:14:55,569 - INFO - Quantization: Enabled
2024-11-16 06:14:55,569 - INFO - Quantization type: 4-bit
2024-11-16 06:14:55,569 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 06:14:55,569 - INFO - Using data directory: ./data
2024-11-16 06:14:55,930 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 06:14:57,989 - INFO - Found 62136 messages
2024-11-16 06:14:58,325 - INFO - Grouped messages into 6845 conversations
2024-11-16 06:14:58,646 - ERROR - Preprocessing error: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating
2024-11-16 06:14:58,795 - ERROR - Preprocessing failed
2024-11-16 06:47:06,478 - INFO - Starting training pipeline:
2024-11-16 06:47:06,479 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 06:47:06,479 - INFO - Output directory: ./llama3_finetuned
2024-11-16 06:47:06,479 - INFO - Batch size: 4
2024-11-16 06:47:06,479 - INFO - Training epochs: 3
2024-11-16 06:47:06,479 - INFO - Max sequence length: 2048
2024-11-16 06:47:06,479 - INFO - Quantization: Enabled
2024-11-16 06:47:06,479 - INFO - Quantization type: 4-bit
2024-11-16 06:47:06,479 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 06:47:06,479 - INFO - Using data directory: ./data
2024-11-16 06:47:06,918 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 07:07:43,775 - INFO - Starting training pipeline:
2024-11-16 07:07:43,775 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 07:07:43,775 - INFO - Output directory: ./llama3_finetuned
2024-11-16 07:07:43,775 - INFO - Batch size: 4
2024-11-16 07:07:43,775 - INFO - Training epochs: 3
2024-11-16 07:07:43,775 - INFO - Max sequence length: 2048
2024-11-16 07:07:43,775 - INFO - Quantization: Enabled
2024-11-16 07:07:43,775 - INFO - Quantization type: 4-bit
2024-11-16 07:07:43,775 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 07:07:43,775 - INFO - Using data directory: ./data
2024-11-16 07:07:44,756 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 07:09:30,418 - INFO - Found 62136 messages
2024-11-16 07:09:31,045 - INFO - Grouped messages into 6845 conversations
2024-11-16 07:09:31,621 - ERROR - Preprocessing error: name 'tokenizer' is not defined
2024-11-16 07:09:31,888 - ERROR - Preprocessing failed
2024-11-16 07:13:03,373 - INFO - Starting training pipeline:
2024-11-16 07:13:03,374 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 07:13:03,374 - INFO - Output directory: ./llama3_finetuned
2024-11-16 07:13:03,374 - INFO - Batch size: 4
2024-11-16 07:13:03,374 - INFO - Training epochs: 3
2024-11-16 07:13:03,374 - INFO - Max sequence length: 2048
2024-11-16 07:13:03,374 - INFO - Quantization: Enabled
2024-11-16 07:13:03,374 - INFO - Quantization type: 4-bit
2024-11-16 07:13:03,375 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 07:13:03,375 - INFO - Using data directory: ./data
2024-11-16 07:13:03,732 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 07:13:05,799 - INFO - Found 62136 messages
2024-11-16 07:13:06,134 - INFO - Grouped messages into 6845 conversations
2024-11-16 07:13:06,444 - ERROR - Preprocessing error: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating
2024-11-16 07:13:06,598 - ERROR - Preprocessing failed
