2024-11-16 05:31:28,748 - INFO - Starting training pipeline:
2024-11-16 05:31:28,748 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 05:31:28,748 - INFO - Output directory: ./llama3_finetuned
2024-11-16 05:31:28,748 - INFO - Batch size: 4
2024-11-16 05:31:28,748 - INFO - Training epochs: 3
2024-11-16 05:31:28,748 - INFO - Max sequence length: 2048
2024-11-16 05:31:28,748 - INFO - Quantization: Enabled
2024-11-16 05:31:28,748 - INFO - Quantization type: 4-bit
2024-11-16 05:31:28,749 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 05:31:28,749 - INFO - Using data directory: ./data
2024-11-16 05:31:29,086 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 05:31:31,275 - INFO - Found 62136 messages
2024-11-16 05:31:31,605 - INFO - Grouped messages into 6845 conversations
2024-11-16 05:31:47,657 - WARNING - Skipped 23 examples due to length constraints
2024-11-16 05:31:47,888 - INFO - Created dataset with 23004 training and 2557 test examples
2024-11-16 05:31:47,896 - INFO - Preprocessing completed successfully
2024-11-16 05:31:47,897 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-16 05:31:48,052 - INFO - Starting model fine-tuning...
2024-11-16 05:31:48,176 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-16 05:31:48,265 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-16 05:31:48,266 - INFO - Compute dtype: torch.bfloat16
2024-11-16 05:31:48,266 - INFO - Quantization config: Enabled
2024-11-16 05:31:48,651 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-16 05:31:50,668 - INFO - Successfully loaded model and tokenizer
2024-11-16 05:31:50,669 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-16 05:31:51,179 - INFO - Processing training dataset...
2024-11-16 05:31:58,244 - INFO - Processing evaluation dataset...
2024-11-16 05:32:01,526 - INFO - Starting training...
2024-11-16 05:32:06,456 - ERROR - Training error: '<=' not supported between instances of 'float' and 'str'
2024-11-16 05:35:41,454 - INFO - Starting training pipeline:
2024-11-16 05:35:41,454 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 05:35:41,454 - INFO - Output directory: ./llama3_finetuned
2024-11-16 05:35:41,454 - INFO - Batch size: 4
2024-11-16 05:35:41,454 - INFO - Training epochs: 3
2024-11-16 05:35:41,454 - INFO - Max sequence length: 2048
2024-11-16 05:35:41,454 - INFO - Quantization: Enabled
2024-11-16 05:35:41,455 - INFO - Quantization type: 4-bit
2024-11-16 05:35:41,455 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 05:35:41,455 - INFO - Using data directory: ./data
2024-11-16 05:35:41,798 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 05:35:43,857 - INFO - Found 62136 messages
2024-11-16 05:35:44,191 - INFO - Grouped messages into 6845 conversations
2024-11-16 05:36:00,278 - WARNING - Skipped 23 examples due to length constraints
2024-11-16 05:36:00,509 - INFO - Created dataset with 23004 training and 2557 test examples
2024-11-16 05:36:00,516 - INFO - Preprocessing completed successfully
2024-11-16 05:36:00,518 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-16 05:36:00,670 - INFO - Starting model fine-tuning...
2024-11-16 05:36:00,786 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-16 05:36:00,825 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-16 05:36:00,825 - INFO - Compute dtype: torch.bfloat16
2024-11-16 05:36:00,825 - INFO - Quantization config: Enabled
2024-11-16 05:36:01,205 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-16 05:36:03,093 - INFO - Successfully loaded model and tokenizer
2024-11-16 05:36:03,093 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-16 05:36:03,587 - INFO - Processing training dataset...
2024-11-16 05:36:10,533 - INFO - Processing evaluation dataset...
2024-11-16 05:36:13,682 - INFO - Starting training...
2024-11-16 05:36:18,669 - ERROR - Training error: '<=' not supported between instances of 'float' and 'str'
2024-11-16 05:53:33,731 - INFO - Starting training pipeline:
2024-11-16 05:53:33,731 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 05:53:33,731 - INFO - Output directory: ./llama3_finetuned
2024-11-16 05:53:33,731 - INFO - Batch size: 4
2024-11-16 05:53:33,731 - INFO - Training epochs: 3
2024-11-16 05:53:33,731 - INFO - Max sequence length: 2048
2024-11-16 05:53:33,732 - INFO - Quantization: Enabled
2024-11-16 05:53:33,732 - INFO - Quantization type: 4-bit
2024-11-16 05:53:33,732 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 05:53:33,732 - INFO - Using data directory: ./data
2024-11-16 05:53:34,115 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 05:53:36,308 - INFO - Found 62136 messages
2024-11-16 05:53:36,670 - INFO - Grouped messages into 6845 conversations
2024-11-16 05:53:52,763 - WARNING - Skipped 23 examples due to length constraints
2024-11-16 05:53:52,996 - INFO - Created dataset with 23004 training and 2557 test examples
2024-11-16 05:53:53,003 - INFO - Preprocessing completed successfully
2024-11-16 05:53:53,005 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-16 05:53:53,162 - INFO - Starting model fine-tuning...
2024-11-16 05:53:53,280 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-16 05:53:53,318 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-16 05:53:53,318 - INFO - Compute dtype: torch.bfloat16
2024-11-16 05:53:53,318 - INFO - Quantization config: Enabled
2024-11-16 05:53:53,688 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-16 05:53:55,669 - INFO - Successfully loaded model and tokenizer
2024-11-16 05:53:55,669 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-16 05:54:00,508 - INFO - Starting training...
2024-11-16 06:14:55,568 - INFO - Starting training pipeline:
2024-11-16 06:14:55,569 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 06:14:55,569 - INFO - Output directory: ./llama3_finetuned
2024-11-16 06:14:55,569 - INFO - Batch size: 4
2024-11-16 06:14:55,569 - INFO - Training epochs: 3
2024-11-16 06:14:55,569 - INFO - Max sequence length: 2048
2024-11-16 06:14:55,569 - INFO - Quantization: Enabled
2024-11-16 06:14:55,569 - INFO - Quantization type: 4-bit
2024-11-16 06:14:55,569 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 06:14:55,569 - INFO - Using data directory: ./data
2024-11-16 06:14:55,930 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 06:14:57,989 - INFO - Found 62136 messages
2024-11-16 06:14:58,325 - INFO - Grouped messages into 6845 conversations
2024-11-16 06:14:58,646 - ERROR - Preprocessing error: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating
2024-11-16 06:14:58,795 - ERROR - Preprocessing failed
2024-11-16 06:47:06,478 - INFO - Starting training pipeline:
2024-11-16 06:47:06,479 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 06:47:06,479 - INFO - Output directory: ./llama3_finetuned
2024-11-16 06:47:06,479 - INFO - Batch size: 4
2024-11-16 06:47:06,479 - INFO - Training epochs: 3
2024-11-16 06:47:06,479 - INFO - Max sequence length: 2048
2024-11-16 06:47:06,479 - INFO - Quantization: Enabled
2024-11-16 06:47:06,479 - INFO - Quantization type: 4-bit
2024-11-16 06:47:06,479 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 06:47:06,479 - INFO - Using data directory: ./data
2024-11-16 06:47:06,918 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 07:07:43,775 - INFO - Starting training pipeline:
2024-11-16 07:07:43,775 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 07:07:43,775 - INFO - Output directory: ./llama3_finetuned
2024-11-16 07:07:43,775 - INFO - Batch size: 4
2024-11-16 07:07:43,775 - INFO - Training epochs: 3
2024-11-16 07:07:43,775 - INFO - Max sequence length: 2048
2024-11-16 07:07:43,775 - INFO - Quantization: Enabled
2024-11-16 07:07:43,775 - INFO - Quantization type: 4-bit
2024-11-16 07:07:43,775 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 07:07:43,775 - INFO - Using data directory: ./data
2024-11-16 07:07:44,756 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 07:09:30,418 - INFO - Found 62136 messages
2024-11-16 07:09:31,045 - INFO - Grouped messages into 6845 conversations
2024-11-16 07:09:31,621 - ERROR - Preprocessing error: name 'tokenizer' is not defined
2024-11-16 07:09:31,888 - ERROR - Preprocessing failed
2024-11-16 07:13:03,373 - INFO - Starting training pipeline:
2024-11-16 07:13:03,374 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 07:13:03,374 - INFO - Output directory: ./llama3_finetuned
2024-11-16 07:13:03,374 - INFO - Batch size: 4
2024-11-16 07:13:03,374 - INFO - Training epochs: 3
2024-11-16 07:13:03,374 - INFO - Max sequence length: 2048
2024-11-16 07:13:03,374 - INFO - Quantization: Enabled
2024-11-16 07:13:03,374 - INFO - Quantization type: 4-bit
2024-11-16 07:13:03,375 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 07:13:03,375 - INFO - Using data directory: ./data
2024-11-16 07:13:03,732 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 07:13:05,799 - INFO - Found 62136 messages
2024-11-16 07:13:06,134 - INFO - Grouped messages into 6845 conversations
2024-11-16 07:13:06,444 - ERROR - Preprocessing error: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating
2024-11-16 07:13:06,598 - ERROR - Preprocessing failed
2024-11-16 17:08:13,993 - INFO - Starting training pipeline:
2024-11-16 17:08:13,993 - INFO - Model: meta-llama/Llama-3.2-3B-Instruct
2024-11-16 17:08:13,993 - INFO - Output directory: ./llama3_finetuned
2024-11-16 17:08:13,993 - INFO - Batch size: 4
2024-11-16 17:08:13,993 - INFO - Training epochs: 3
2024-11-16 17:08:13,993 - INFO - Max sequence length: 2048
2024-11-16 17:08:13,994 - INFO - Quantization: Enabled
2024-11-16 17:08:13,994 - INFO - Quantization type: 4-bit
2024-11-16 17:08:13,994 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B-Instruct
2024-11-16 17:08:13,994 - INFO - Using data directory: ./data
2024-11-16 17:08:14,902 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 17:08:23,312 - INFO - Found 62136 messages
2024-11-16 17:08:23,963 - INFO - Grouped messages into 6845 conversations
2024-11-16 17:08:31,789 - WARNING - Skipped 5 examples due to length constraints
2024-11-16 17:08:31,917 - INFO - Created dataset with 6156 training and 684 test examples
2024-11-16 17:08:31,919 - INFO - Preprocessing completed successfully
2024-11-16 17:08:31,921 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-16 17:08:32,123 - INFO - Starting model fine-tuning...
2024-11-16 17:08:32,295 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-16 17:08:32,365 - INFO - Loading model meta-llama/Llama-3.2-3B-Instruct
2024-11-16 17:08:32,365 - INFO - Compute dtype: torch.bfloat16
2024-11-16 17:08:32,365 - INFO - Quantization config: Enabled
2024-11-16 17:18:51,295 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-16 17:18:56,266 - INFO - Successfully loaded model and tokenizer
2024-11-16 17:18:56,267 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-16 17:18:58,105 - INFO - Starting training...
2024-11-16 17:47:26,249 - INFO - Training completed. Results: TrainOutput(global_step=700, training_loss=1.8548852007729666, metrics={'train_runtime': 1706.9107, 'train_samples_per_second': 10.82, 'train_steps_per_second': 0.675, 'total_flos': 5.60738828758057e+16, 'train_loss': 1.8548852007729666, 'epoch': 1.8193632228719947})
2024-11-16 17:47:26,708 - INFO - Merging and saving model...
2024-11-16 17:47:35,959 - INFO - Training completed. Model saved to: ./llama3_finetuned/final_model
2024-11-16 17:47:35,960 - INFO - Training completed successfully:
2024-11-16 17:47:35,960 - INFO - Model size: 2.84 GB
2024-11-16 17:47:35,960 - INFO - Number of shards: 0
2024-11-16 19:39:05,223 - INFO - Starting training pipeline:
2024-11-16 19:39:05,223 - INFO - Model: meta-llama/Llama-3.2-3B-Instruct
2024-11-16 19:39:05,223 - INFO - Output directory: ./llama3_finetuned
2024-11-16 19:39:05,224 - INFO - Batch size: 4
2024-11-16 19:39:05,224 - INFO - Training epochs: 3
2024-11-16 19:39:05,224 - INFO - Max sequence length: 2048
2024-11-16 19:39:05,224 - INFO - Quantization: Enabled
2024-11-16 19:39:05,224 - INFO - Quantization type: 4-bit
2024-11-16 19:39:05,224 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B-Instruct
2024-11-16 19:39:05,224 - INFO - Using data directory: ./data
2024-11-16 19:39:06,062 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 19:39:29,444 - INFO - Found 62136 messages
2024-11-16 19:39:30,090 - INFO - Grouped messages into 6845 conversations
2024-11-16 19:39:42,493 - WARNING - Skipped 5 examples due to length constraints
2024-11-16 19:39:42,720 - INFO - Created dataset with 6156 training and 684 test examples
2024-11-16 19:39:42,723 - INFO - Preprocessing completed successfully
2024-11-16 19:39:42,729 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-16 19:39:42,933 - INFO - Starting model fine-tuning...
2024-11-16 19:39:43,108 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-16 19:39:43,172 - INFO - Loading model meta-llama/Llama-3.2-3B-Instruct
2024-11-16 19:39:43,172 - INFO - Compute dtype: torch.bfloat16
2024-11-16 19:39:43,172 - INFO - Quantization config: Enabled
2024-11-16 19:39:43,869 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-16 19:40:37,897 - INFO - Successfully loaded model and tokenizer
2024-11-16 19:40:37,897 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-16 19:40:38,477 - INFO - Formatting dataset with chatml format
2024-11-16 19:40:40,702 - INFO - Starting training...
2024-11-16 19:45:12,495 - INFO - Starting training pipeline:
2024-11-16 19:45:12,495 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 19:45:12,495 - INFO - Output directory: ./llama3_finetuned
2024-11-16 19:45:12,495 - INFO - Batch size: 4
2024-11-16 19:45:12,495 - INFO - Training epochs: 3
2024-11-16 19:45:12,495 - INFO - Max sequence length: 2048
2024-11-16 19:45:12,495 - INFO - Quantization: Enabled
2024-11-16 19:45:12,495 - INFO - Quantization type: 4-bit
2024-11-16 19:45:12,496 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 19:45:12,496 - INFO - Using data directory: ./data
2024-11-16 19:45:12,864 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 19:45:14,952 - INFO - Found 62136 messages
2024-11-16 19:45:15,292 - INFO - Grouped messages into 6845 conversations
2024-11-16 19:45:15,614 - INFO - Set base model chat template
2024-11-16 19:45:18,885 - WARNING - Skipped 5 examples due to length constraints
2024-11-16 19:45:19,021 - INFO - Created dataset with 6156 training and 684 test examples
2024-11-16 19:45:19,023 - INFO - Preprocessing completed successfully
2024-11-16 19:45:19,024 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-16 19:45:19,177 - INFO - Starting model fine-tuning...
2024-11-16 19:45:19,294 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-16 19:45:19,339 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-16 19:45:19,339 - INFO - Compute dtype: torch.bfloat16
2024-11-16 19:45:19,339 - INFO - Quantization config: Enabled
2024-11-16 19:45:19,726 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-16 19:45:21,715 - INFO - Successfully loaded model and tokenizer
2024-11-16 19:45:21,715 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-16 19:45:22,237 - INFO - Formatting dataset with chatml format
2024-11-16 19:45:22,395 - ERROR - Training error: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating
2024-11-16 19:59:16,506 - INFO - Starting training pipeline:
2024-11-16 19:59:16,506 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 19:59:16,506 - INFO - Output directory: ./llama3_finetuned
2024-11-16 19:59:16,506 - INFO - Batch size: 4
2024-11-16 19:59:16,506 - INFO - Training epochs: 3
2024-11-16 19:59:16,506 - INFO - Max sequence length: 2048
2024-11-16 19:59:16,507 - INFO - Quantization: Enabled
2024-11-16 19:59:16,507 - INFO - Quantization type: 4-bit
2024-11-16 19:59:16,507 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 19:59:16,507 - INFO - Using data directory: ./data
2024-11-16 19:59:16,876 - INFO - Set pad_token to eos_token for tokenizer
2024-11-16 19:59:18,969 - INFO - Found 62136 messages
2024-11-16 19:59:19,307 - INFO - Grouped messages into 6845 conversations
2024-11-16 19:59:19,607 - INFO - Set base model chat template
2024-11-16 19:59:22,917 - WARNING - Skipped 5 examples due to length constraints
2024-11-16 19:59:23,054 - INFO - Created dataset with 6156 training and 684 test examples
2024-11-16 19:59:23,056 - INFO - Preprocessing completed successfully
2024-11-16 19:59:23,057 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-16 19:59:23,215 - INFO - Starting model fine-tuning...
2024-11-16 19:59:23,333 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-16 19:59:23,390 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-16 19:59:23,390 - INFO - Compute dtype: torch.bfloat16
2024-11-16 19:59:23,390 - INFO - Quantization config: Enabled
2024-11-16 19:59:23,776 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-16 19:59:26,182 - INFO - Successfully loaded model and tokenizer
2024-11-16 19:59:26,183 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-16 19:59:26,716 - INFO - Formatting dataset with chatml format
2024-11-16 19:59:26,873 - ERROR - Training error: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating
2024-11-16 20:13:15,459 - INFO - Starting training pipeline:
2024-11-16 20:13:15,459 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-16 20:13:15,459 - INFO - Output directory: ./llama3_finetuned
2024-11-16 20:13:15,459 - INFO - Batch size: 4
2024-11-16 20:13:15,460 - INFO - Training epochs: 3
2024-11-16 20:13:15,460 - INFO - Max sequence length: 2048
2024-11-16 20:13:15,460 - INFO - Quantization: Enabled
2024-11-16 20:13:15,460 - INFO - Quantization type: 4-bit
2024-11-16 20:13:15,460 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-16 20:13:15,460 - INFO - Using data directory: ./data
2024-11-16 20:13:15,460 - INFO - Initializing tokenizer for meta-llama/Llama-3.2-3B
2024-11-16 20:13:15,882 - INFO - Set pad_token to eos_token
2024-11-16 20:13:15,882 - INFO - Set base model chat template
2024-11-16 20:13:18,228 - INFO - Found 62136 messages
2024-11-16 20:13:18,564 - INFO - Grouped messages into 6845 conversations
2024-11-16 20:13:18,863 - INFO - Formatted 6845 conversations
2024-11-16 20:13:22,086 - WARNING - Skipped 5 examples due to length constraints or processing errors
2024-11-16 20:13:22,224 - INFO - Created dataset with 6156 training and 684 test examples
2024-11-16 20:13:22,226 - INFO - Preprocessing completed successfully
2024-11-16 20:13:22,227 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-16 20:13:22,382 - INFO - Starting model fine-tuning...
2024-11-16 20:13:22,497 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-16 20:13:22,554 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-16 20:13:22,554 - INFO - Compute dtype: torch.bfloat16
2024-11-16 20:13:22,554 - INFO - Quantization config: Enabled
2024-11-16 20:13:22,929 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-16 20:13:24,548 - INFO - Initializing tokenizer for meta-llama/Llama-3.2-3B
2024-11-16 20:13:24,853 - INFO - Set pad_token to eos_token
2024-11-16 20:13:24,854 - INFO - Set base model chat template
2024-11-16 20:13:24,854 - INFO - Successfully loaded model and tokenizer
2024-11-16 20:13:24,854 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-16 20:13:25,426 - INFO - Formatting dataset with chatml format
2024-11-16 20:13:27,333 - INFO - Starting training...
