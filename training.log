2024-11-17 00:21:32,715 - INFO - Starting training pipeline:
2024-11-17 00:21:32,715 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-17 00:21:32,715 - INFO - Output directory: ./llama3_finetuned
2024-11-17 00:21:32,715 - INFO - Batch size: 4
2024-11-17 00:21:32,715 - INFO - Training epochs: 3
2024-11-17 00:21:32,715 - INFO - Max sequence length: 2048
2024-11-17 00:21:32,715 - INFO - Quantization: Enabled
2024-11-17 00:21:32,715 - INFO - Quantization type: 4-bit
2024-11-17 00:21:32,716 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-17 00:21:32,716 - INFO - Using data directory: ./data
2024-11-17 00:21:32,716 - INFO - Initializing tokenizer for meta-llama/Llama-3.2-3B
2024-11-17 00:21:33,076 - INFO - Set pad_token to eos_token
2024-11-17 00:21:33,076 - INFO - Set base model chat template
2024-11-17 00:21:35,138 - INFO - Found 62136 messages
2024-11-17 00:21:35,473 - INFO - Grouped messages into 6845 conversations
2024-11-17 00:21:35,768 - INFO - Formatted 6845 conversations
2024-11-17 00:21:38,911 - WARNING - Skipped 5 examples due to length constraints or processing errors
2024-11-17 00:21:39,044 - INFO - Created dataset with 6156 training and 684 test examples
2024-11-17 00:21:39,046 - INFO - Preprocessing completed successfully
2024-11-17 00:21:39,047 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-17 00:21:39,195 - INFO - Starting model fine-tuning...
2024-11-17 00:21:39,308 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-17 00:21:39,413 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-17 00:21:39,413 - INFO - Compute dtype: torch.bfloat16
2024-11-17 00:21:39,413 - INFO - Quantization config: Enabled
2024-11-17 00:21:41,915 - INFO - Initializing tokenizer for meta-llama/Llama-3.2-3B
2024-11-17 00:21:42,215 - INFO - Set pad_token to eos_token
2024-11-17 00:21:42,215 - INFO - Set base model chat template
2024-11-17 00:21:42,215 - INFO - Successfully loaded model and tokenizer
2024-11-17 00:21:42,215 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-17 00:21:42,897 - INFO - Formatting dataset with chatml format
2024-11-17 00:21:44,654 - INFO - Starting training...
2024-11-17 00:21:45,688 - ERROR - Training error: You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`
2024-11-17 00:24:49,619 - INFO - Starting training pipeline:
2024-11-17 00:24:49,619 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-17 00:24:49,619 - INFO - Output directory: ./llama3_finetuned
2024-11-17 00:24:49,620 - INFO - Batch size: 4
2024-11-17 00:24:49,620 - INFO - Training epochs: 3
2024-11-17 00:24:49,620 - INFO - Max sequence length: 2048
2024-11-17 00:24:49,620 - INFO - Quantization: Enabled
2024-11-17 00:24:49,620 - INFO - Quantization type: 4-bit
2024-11-17 00:24:49,620 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-17 00:24:49,620 - INFO - Using data directory: ./data
2024-11-17 00:24:49,620 - INFO - Initializing tokenizer for meta-llama/Llama-3.2-3B
2024-11-17 00:24:49,998 - INFO - Set pad_token to eos_token
2024-11-17 00:24:49,999 - INFO - Set base model chat template
2024-11-17 00:24:52,074 - INFO - Found 62136 messages
2024-11-17 00:24:52,437 - INFO - Grouped messages into 6845 conversations
2024-11-17 00:24:52,760 - INFO - Formatted 6845 conversations
2024-11-17 00:24:55,851 - WARNING - Skipped 5 examples due to length constraints or processing errors
2024-11-17 00:24:55,984 - INFO - Created dataset with 6156 training and 684 test examples
2024-11-17 00:24:55,986 - INFO - Preprocessing completed successfully
2024-11-17 00:24:55,987 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-17 00:24:56,134 - INFO - Starting model fine-tuning...
2024-11-17 00:24:56,249 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-17 00:24:56,459 - INFO - Found 4 GPUs
2024-11-17 00:24:56,459 - INFO - Multiple GPUs detected - disabling quantization
2024-11-17 00:24:56,459 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-17 00:24:56,459 - INFO - Compute dtype: torch.bfloat16
2024-11-17 00:24:56,460 - INFO - Quantization config: Disabled
2024-11-17 00:24:58,389 - INFO - Initializing tokenizer for meta-llama/Llama-3.2-3B
2024-11-17 00:24:58,679 - INFO - Set pad_token to eos_token
2024-11-17 00:24:58,679 - INFO - Set base model chat template
2024-11-17 00:24:58,679 - INFO - Successfully loaded model and tokenizer
2024-11-17 00:24:58,679 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-17 00:24:59,404 - INFO - Formatting dataset with chatml format
2024-11-17 00:25:01,183 - INFO - Starting training...
2024-11-17 00:25:05,064 - ERROR - Training error: element 0 of tensors does not require grad and does not have a grad_fn
2024-11-17 00:27:53,846 - INFO - Starting training pipeline:
2024-11-17 00:27:53,846 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-17 00:27:53,846 - INFO - Output directory: ./llama3_finetuned
2024-11-17 00:27:53,846 - INFO - Batch size: 4
2024-11-17 00:27:53,846 - INFO - Training epochs: 3
2024-11-17 00:27:53,846 - INFO - Max sequence length: 2048
2024-11-17 00:27:53,847 - INFO - Quantization: Enabled
2024-11-17 00:27:53,847 - INFO - Quantization type: 4-bit
2024-11-17 00:27:53,847 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-17 00:27:53,847 - INFO - Using data directory: ./data
2024-11-17 00:27:53,847 - INFO - Initializing tokenizer for meta-llama/Llama-3.2-3B
2024-11-17 00:27:54,194 - INFO - Set pad_token to eos_token
2024-11-17 00:27:54,194 - INFO - Set base model chat template
2024-11-17 00:27:56,260 - INFO - Found 62136 messages
2024-11-17 00:27:56,600 - INFO - Grouped messages into 6845 conversations
2024-11-17 00:27:56,898 - INFO - Formatted 6845 conversations
2024-11-17 00:28:00,050 - WARNING - Skipped 5 examples due to length constraints or processing errors
2024-11-17 00:28:00,183 - INFO - Created dataset with 6156 training and 684 test examples
2024-11-17 00:28:00,185 - INFO - Preprocessing completed successfully
2024-11-17 00:28:00,186 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-17 00:28:00,337 - INFO - Starting model fine-tuning...
2024-11-17 00:28:00,447 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-17 00:28:00,600 - INFO - Found 4 GPUs
2024-11-17 00:28:00,600 - INFO - Multiple GPUs detected - disabling quantization
2024-11-17 00:28:00,600 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-17 00:28:00,600 - INFO - Compute dtype: torch.bfloat16
2024-11-17 00:28:00,600 - INFO - Quantization config: Disabled
2024-11-17 00:28:02,512 - INFO - Initializing tokenizer for meta-llama/Llama-3.2-3B
2024-11-17 00:28:02,823 - INFO - Set pad_token to eos_token
2024-11-17 00:28:02,824 - INFO - Set base model chat template
2024-11-17 00:28:02,824 - INFO - Successfully loaded model and tokenizer
2024-11-17 00:28:02,824 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-17 00:28:03,610 - INFO - Formatting dataset with chatml format
2024-11-17 00:28:05,394 - INFO - Starting training...
2024-11-17 00:34:03,630 - INFO - Starting training pipeline:
2024-11-17 00:34:03,630 - INFO - Model: meta-llama/Llama-3.2-3B
2024-11-17 00:34:03,630 - INFO - Output directory: ./llama3_finetuned
2024-11-17 00:34:03,630 - INFO - Batch size: 4
2024-11-17 00:34:03,631 - INFO - Training epochs: 3
2024-11-17 00:34:03,631 - INFO - Max sequence length: 2048
2024-11-17 00:34:03,631 - INFO - Quantization: Enabled
2024-11-17 00:34:03,631 - INFO - Quantization type: 4-bit
2024-11-17 00:34:03,631 - INFO - Starting preprocessing with model: meta-llama/Llama-3.2-3B
2024-11-17 00:34:03,631 - INFO - Using data directory: ./data
2024-11-17 00:34:03,631 - INFO - Initializing tokenizer for meta-llama/Llama-3.2-3B
2024-11-17 00:34:04,028 - INFO - Set pad_token to eos_token
2024-11-17 00:34:04,029 - INFO - Set base model chat template
2024-11-17 00:34:06,441 - INFO - Found 62136 messages
2024-11-17 00:34:06,799 - INFO - Grouped messages into 6845 conversations
2024-11-17 00:34:07,126 - INFO - Formatted 6845 conversations
2024-11-17 00:34:10,917 - WARNING - Skipped 5 examples due to length constraints or processing errors
2024-11-17 00:34:11,063 - INFO - Created dataset with 6156 training and 684 test examples
2024-11-17 00:34:11,067 - INFO - Preprocessing completed successfully
2024-11-17 00:34:11,070 - INFO - Saved dataset sample to llama3_finetuned/dataset_sample.log
2024-11-17 00:34:11,241 - INFO - Starting model fine-tuning...
2024-11-17 00:34:11,361 - INFO - Saved training configuration to ./llama3_finetuned/training_config.json
2024-11-17 00:34:11,422 - INFO - Found 1 GPUs
2024-11-17 00:34:11,423 - INFO - Loading model meta-llama/Llama-3.2-3B
2024-11-17 00:34:11,424 - INFO - Compute dtype: torch.bfloat16
2024-11-17 00:34:11,424 - INFO - Quantization config: Enabled
2024-11-17 00:34:12,037 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 00:35:03,635 - INFO - Initializing tokenizer for meta-llama/Llama-3.2-3B
2024-11-17 00:35:03,944 - INFO - Set pad_token to eos_token
2024-11-17 00:35:03,944 - INFO - Set base model chat template
2024-11-17 00:35:03,944 - INFO - Successfully loaded model and tokenizer
2024-11-17 00:35:03,945 - INFO - Configuring LoRA with target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2024-11-17 00:35:04,482 - INFO - Formatting dataset with chatml format
2024-11-17 00:35:06,352 - INFO - Starting training...
2024-11-17 01:02:55,444 - INFO - Training completed. Results: TrainOutput(global_step=700, training_loss=2.0513160051618304, metrics={'train_runtime': 1668.0765, 'train_samples_per_second': 11.071, 'train_steps_per_second': 0.691, 'total_flos': 4.935431548408627e+16, 'train_loss': 2.0513160051618304, 'epoch': 1.8193632228719947})
2024-11-17 01:02:55,908 - INFO - Merging and saving model...
2024-11-17 01:03:03,479 - INFO - Training completed. Model saved to: ./llama3_finetuned/final_model
2024-11-17 01:03:03,480 - INFO - Training completed successfully:
2024-11-17 01:03:03,480 - INFO - Model size: 2.84 GB
2024-11-17 01:03:03,480 - INFO - Number of shards: 0
2024-11-17 01:03:35,948 - ERROR - Error loading model: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./llama3_finetuned.
2024-11-17 01:03:35,948 - ERROR - Failed to load model or tokenizer
2024-11-17 01:04:26,509 - ERROR - Error loading model: Model path does not exist: llama_finetuned_final_model
2024-11-17 01:04:26,509 - ERROR - Failed to load model or tokenizer
2024-11-17 01:04:41,085 - ERROR - Error loading model: Model path does not exist: llama_finetuned/final_model
2024-11-17 01:04:41,085 - ERROR - Failed to load model or tokenizer
2024-11-17 01:05:22,707 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:05:23,243 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:05:25,021 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:05:25,452 - INFO - Model loaded with device map: auto
2024-11-17 01:05:25,452 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:13:11,961 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:13:12,485 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:13:13,655 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:13:14,131 - INFO - Model loaded with device map: auto
2024-11-17 01:13:14,131 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:30:54,562 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:30:55,082 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:30:56,138 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:30:56,569 - INFO - Model loaded with device map: auto
2024-11-17 01:30:56,569 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:31:05,019 - ERROR - Chat error: The following `model_kwargs` are not used by the model: ['stop_sequences'] (note: typos in the generate arguments will also show up in this list)
2024-11-17 01:36:17,490 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:36:18,004 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:36:19,116 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:36:19,559 - INFO - Model loaded with device map: auto
2024-11-17 01:36:19,559 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:36:28,582 - ERROR - Chat error: name 'tokenizer' is not defined
2024-11-17 01:39:16,775 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:39:17,282 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:39:18,354 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:39:18,877 - INFO - Model loaded with device map: auto
2024-11-17 01:39:18,877 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:40:34,383 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:40:34,897 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:40:35,935 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:40:36,366 - INFO - Model loaded with device map: auto
2024-11-17 01:40:36,366 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:43:51,302 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:43:51,820 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:43:52,859 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:43:53,347 - INFO - Model loaded with device map: auto
2024-11-17 01:43:53,347 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:45:13,812 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:45:14,345 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:45:15,425 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:45:15,858 - INFO - Model loaded with device map: auto
2024-11-17 01:45:15,858 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:47:21,596 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:47:22,109 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:47:23,164 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:47:23,664 - INFO - Model loaded with device map: auto
2024-11-17 01:47:23,664 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:49:35,357 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:49:35,869 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:49:36,920 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:49:37,348 - INFO - Model loaded with device map: auto
2024-11-17 01:49:37,348 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:53:43,262 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:53:43,852 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:53:44,902 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:53:45,360 - INFO - Model loaded with device map: auto
2024-11-17 01:53:45,360 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:56:47,375 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:56:47,878 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:56:49,022 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:56:49,444 - INFO - Model loaded with device map: auto
2024-11-17 01:56:49,444 - INFO - Model dtype: torch.bfloat16
2024-11-17 01:58:22,054 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 01:58:22,572 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 01:58:23,684 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 01:58:24,146 - INFO - Model loaded with device map: auto
2024-11-17 01:58:24,146 - INFO - Model dtype: torch.bfloat16
2024-11-17 02:03:26,667 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 02:03:27,170 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 02:03:28,239 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 02:03:28,765 - INFO - Model loaded with device map: auto
2024-11-17 02:03:28,765 - INFO - Model dtype: torch.bfloat16
2024-11-17 02:04:50,390 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 02:04:50,909 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 02:04:51,956 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 02:04:52,383 - INFO - Model loaded with device map: auto
2024-11-17 02:04:52,383 - INFO - Model dtype: torch.bfloat16
2024-11-17 02:07:53,879 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 02:07:54,400 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 02:07:55,442 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 02:07:55,876 - INFO - Model loaded with device map: auto
2024-11-17 02:07:55,876 - INFO - Model dtype: torch.bfloat16
2024-11-17 02:10:00,751 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 02:10:01,264 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 02:10:02,310 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 02:10:02,746 - INFO - Model loaded with device map: auto
2024-11-17 02:10:02,746 - INFO - Model dtype: torch.bfloat16
2024-11-17 02:10:36,082 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 02:10:36,590 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 02:10:37,633 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 02:10:38,077 - INFO - Model loaded with device map: auto
2024-11-17 02:10:38,077 - INFO - Model dtype: torch.bfloat16
2024-11-17 02:12:18,391 - INFO - Loaded model card: meta-llama/Llama-3.2-3B
2024-11-17 02:12:18,964 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-11-17 02:12:20,001 - INFO - Initializing tokenizer for llama3_finetuned/final_model
2024-11-17 02:12:20,429 - INFO - Model loaded with device map: auto
2024-11-17 02:12:20,429 - INFO - Model dtype: torch.bfloat16
