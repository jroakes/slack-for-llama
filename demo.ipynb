{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install repocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available options:\n",
      "1. Code Review. Action: code-review\n",
      "2. Code Improvement. Action: code-improvement\n",
      "3. Code Completion. Action: code-completion\n",
      "4. Code Correction. Action: code-correction\n",
      "5. Custom Action. Action: <your custom action>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "After analyzing the code and the error message, I found that there's a syntax error in `preprocess.py` that's causing the chat template setting to fail. Here are the changes needed:\n",
       "\n",
       "File Path: ./lib/preprocess.py\n",
       "\n",
       "Changes:\n",
       "- Fixed syntax error in `create_dataset` function definition (removed stray 'd' character)\n",
       "- Fixed escape sequences in the chat template string\n",
       "- Added error handling for chat template setting\n",
       "\n",
       "Updated Code:\n",
       "\n",
       "```\n",
       "\"\"\"Preprocess Slack conversations for Llama 3 fine-tuning.\"\"\"\n",
       "[Previous code remains unchanged until create_dataset function]\n",
       "\n",
       "def create_dataset(\n",
       "    conversation_data: List[Dict],\n",
       "    tokenizer: AutoTokenizer,\n",
       "    max_length: int = 2048\n",
       ") -> Dataset:\n",
       "    \"\"\"Create dataset with length validation.\"\"\"\n",
       "    # Set base model template if not set\n",
       "    try:\n",
       "        if not tokenizer.chat_template:\n",
       "            base_template = \"<|begin_of_text|>{% for message in messages %}{% if message.role == 'system' %}System: {{ message.content }}\\n\\n{% elif message.role == 'user' %}Human: {{ message.content }}\\n{% elif message.role == 'assistant' %}Assistant: {{ message.content }}\\n\\n{% endif %}{% endfor %}\"\n",
       "            tokenizer.chat_template = base_template\n",
       "            logging.info(\"Set base model chat template\")\n",
       "    except Exception as e:\n",
       "        logging.error(f\"Error setting chat template: {str(e)}\")\n",
       "        raise\n",
       "    \n",
       "    filtered_examples = []\n",
       "    skipped_count = 0\n",
       "    \n",
       "    # Process each conversation\n",
       "    for conv_data in conversation_data:\n",
       "        # Format using tokenizer's template\n",
       "        text = tokenizer.apply_chat_template(\n",
       "            conv_data[\"messages\"],\n",
       "            tokenize=False\n",
       "        )\n",
       "        \n",
       "        # Validate length\n",
       "        tokens = tokenizer.encode(text)\n",
       "        if len(tokens) <= max_length:\n",
       "            filtered_examples.append({\n",
       "                \"text\": text,\n",
       "                \"messages\": conv_data[\"messages\"],\n",
       "                \"length\": len(conv_data[\"messages\"])\n",
       "            })\n",
       "        else:\n",
       "            skipped_count += 1\n",
       "    \n",
       "    if skipped_count > 0:\n",
       "        logging.warning(f\"Skipped {skipped_count} examples due to length constraints\")\n",
       "    \n",
       "    # Create dataset\n",
       "    dataset = Dataset.from_dict({\n",
       "        \"text\": [ex[\"text\"] for ex in filtered_examples],\n",
       "        \"messages\": [ex[\"messages\"] for ex in filtered_examples],\n",
       "        \"length\": [ex[\"length\"] for ex in filtered_examples]\n",
       "    })\n",
       "    \n",
       "    # Split and log\n",
       "    dataset = dataset.train_test_split(test_size=0.1)\n",
       "    logging.info(f\"Created dataset with {len(dataset['train'])} training and {len(dataset['test'])} test examples\")\n",
       "    \n",
       "    return dataset\n",
       "\n",
       "[Remaining code remains unchanged]\n",
       "```\n",
       "\n",
       "The main issues fixed were:\n",
       "1. Removed a stray 'd' character that was breaking the function definition\n",
       "2. Fixed the escape sequences in the chat template string (removed double escaping)\n",
       "3. Added error handling around the chat template setting to better catch and report issues\n",
       "\n",
       "These changes should resolve the chat template error you're encountering. The template will now be properly set for both base and instruction-tuned models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from repocoder import send_for_review, print_options\n",
    "\n",
    "print_options()\n",
    "\n",
    "action = \"\"\"\n",
    "It looks like in processing.py, we are setting the tokenizer.chat_template if it is not set, but I am still getting the following error when training:\n",
    "2024-11-16 19:59:26,873 - ERROR - Training error: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "\n",
    "Here is the code where the chat_template is set:\n",
    "```\n",
    "def create_dataset(\n",
    "    conversation_data: List[Dict],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    max_length: int = 2048\n",
    ") -> Dataset:\n",
    "    # Set base model template if not set\n",
    "    if not tokenizer.chat_template:\n",
    "        base_template = \"<|begin_of_text|>{% for message in messages %}{% if message.role == 'system' %}System: {{ message.content }}\\\\n\\\\n{% elif message.role == 'user' %}Human: {{ message.content }}\\\\n{% elif message.role == 'assistant' %}Assistant: {{ message.content }}\\\\n\\\\n{% endif %}{% endfor %}\"\n",
    "        tokenizer.chat_template = base_template\n",
    "        logging.info(\"Set base model chat template\")\n",
    "```\n",
    "\n",
    "Is there a separate tokenizer defined eleswhere that still does not have this set, or can you see any other issues for this.\n",
    "\n",
    "The model: meta-llama/Llama-3.2-3B would not have the teamplte set because it is a base model.\n",
    "The model: meta-llama/Llama-3.2-3B-Instruct would have it set because it is a fine-tuned model.\n",
    "\n",
    "My earlier instructions were to simply check, after the tokenizer was defined, whether there was a chat_template, and if not, define one.  THis way it would cover both models.\n",
    "\n",
    "\"\"\"\n",
    "# model=\"claude-3-5-sonnet-latest\"\n",
    "\n",
    "send_for_review(action, llm=\"anthropic\", model=\"claude-3-5-sonnet-latest\", additional_exclude_files=['.gitignore', \"LICENSE.md\"], additional_exclude_dirs=[\"data\", \".idea\", \".ipython\", \".nv\", \".lightning_studio\", \".vscode\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_template = \"<|begin_of_text|>{% for message in messages %}{% if message.role == 'system' %}System: {{ message.content }}\\\\n\\\\n{% elif message.role == 'user' %}Human: {{ message.content }}\\\\n{% elif message.role == 'assistant' %}Assistant: {{ message.content }}\\\\n\\\\n{% endif %}{% endfor %}\"\n",
    "tokenizer.chat_template = base_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|>{% for message in messages %}{% if message.role == 'system' %}System: {{ message.content }}\\\\n\\\\n{% elif message.role == 'user' %}Human: {{ message.content }}\\\\n{% elif message.role == 'assistant' %}Assistant: {{ message.content }}\\\\n\\\\n{% endif %}{% endfor %}\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_chat_template()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
