{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install repocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repocoder import send_for_review, print_options\n",
    "\n",
    "print_options()\n",
    "\n",
    "action = \"\"\"\n",
    "There is something wrong with the stopping criteria.\n",
    "Here is a chat:\n",
    "```\n",
    "You: Who is John Mueller?\n",
    "\n",
    "Assistant: Assistant\n",
    "```\n",
    "\n",
    "The response is this if I take out the stopping criteria:\n",
    "Assistant: Assistant: Heâ€™s a man who likes to talk about things that are not his responsibility.\\n\\t\\t\\tOriginal Source:\\n\\thttps://www.searchenginejournal.com/author/john-mueller/\\n\\n\\n\\nHuman :smile:\\nAssistant :thumbsup:\\n\\nHumans :trollface:\\nAI :stuck_out_tongue_winking_eye:\\n@Patrick Stox I'm sorry, but we're talking about John Mueller her\n",
    "\n",
    "I think the ChatStoppingCriteria is stopping on the \":\" rather than \"Human:\", \"human:\"\n",
    "\"\"\"\n",
    "\n",
    "# model=\"claude-3-5-sonnet-latest\"\n",
    "# model=\"gemini-exp-1114\"\n",
    "\n",
    "send_for_review(action, llm=\"anthropic\", model=\"claude-3-5-sonnet-latest\", additional_exclude_files=['.gitignore', \"LICENSE.md\"], additional_exclude_dirs=[\"data\", \".idea\", \".ipython\", \".nv\", \".lightning_studio\", \".vscode\"])\n",
    "#send_for_review(action, llm=\"gemini\", model=\"gemini-exp-1114\", additional_exclude_files=['.gitignore', \"LICENSE.md\"], additional_exclude_dirs=[\"data\", \".idea\", \".ipython\", \".nv\", \".lightning_studio\", \".vscode\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_template = \"<|begin_of_text|>{% for message in messages %}{% if message.role == 'system' %}System: {{ message.content }}\\\\n\\\\n{% elif message.role == 'user' %}Human: {{ message.content }}\\\\n{% elif message.role == 'assistant' %}Assistant: {{ message.content }}\\\\n\\\\n{% endif %}{% endfor %}\"\n",
    "tokenizer.chat_template = base_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|>{% for message in messages %}{% if message.role == 'system' %}System: {{ message.content }}\\\\n\\\\n{% elif message.role == 'user' %}Human: {{ message.content }}\\\\n{% elif message.role == 'assistant' %}Assistant: {{ message.content }}\\\\n\\\\n{% endif %}{% endfor %}\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_chat_template()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
